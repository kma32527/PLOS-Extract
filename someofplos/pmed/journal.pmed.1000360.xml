<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="discussion" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="nlm-ta">PLoS Med</journal-id><journal-id journal-id-type="publisher-id">PLoS</journal-id><journal-id journal-id-type="pmc">plosmed</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Medicine</journal-title></journal-title-group><issn pub-type="epub">1549-1676</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">10-PLME-ES-5312R2</article-id><article-id pub-id-type="doi">10.1371/journal.pmed.1000360</article-id><article-categories><subj-group subj-group-type="heading"><subject>Essay</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computer Science/Ontology and Logics</subject></subj-group></article-categories><title-group><article-title>Why Do Evaluations of eHealth Programs Fail? An Alternative Set of Guiding Principles</article-title></title-group><contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Greenhalgh</surname><given-names>Trisha</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Russell</surname><given-names>Jill</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
</contrib-group><aff id="aff1"><label>1</label><addr-line>Healthcare Innovation and Policy Unit, Centre for Health Sciences, Barts and The London School of Medicine and Dentistry, London, United Kingdom</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Division of Medical Education, University College London, London, United Kingdom</addr-line>       </aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">p.greenhalgh@qmul.ac.uk</email></corresp>
<fn fn-type="con"><p><ext-link ext-link-type="uri" xlink:href="http://www.icmje.org/" xlink:type="simple">ICMJE</ext-link> criteria for authorship read and met: TG JR. Agree with the manuscript's results and conclusions: TG JR. Wrote the first draft of the paper: TG. Contributed to the writing of the paper: TG JR.</p></fn>
<fn><p>The Essay section contains opinion pieces on topics of broad interest to a general medical audience.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>11</month><year>2010</year></pub-date><pub-date pub-type="epub"><day>2</day><month>11</month><year>2010</year></pub-date><volume>7</volume><issue>11</issue><elocation-id>e1000360</elocation-id><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2010</copyright-year><copyright-holder>Greenhalgh, Russell</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract abstract-type="toc">
<p>Trisha Greenhalgh and Jill Russell discuss the relative merits of “scientific” and “social practice” approaches to evaluation and argue that eHealth evaluation is in need of a paradigm shift.</p>
</abstract><funding-group><funding-statement>The ideas in this paper were developed during our independent evaluation of the UK Summary Care Record programme, funded by a research grant from the UK National Institute of Health Research (ref CFHEP002 and 007) and a study funded by the UK Medical Research Council (“Healthcare Electronic Records in Organisations”, ref 07/133). The funders had no role in the preparation of this manuscript or the decision to publish.</funding-statement></funding-group><counts><page-count count="5"/></counts></article-meta>
</front>
<body><boxed-text id="pmed-1000360-box001" position="float"><sec id="s1a">
<title>Summary Points</title>
<list list-type="bullet"><list-item>
<p>We argue that the assumptions, methods, and study designs of experimental science, whilst useful in many contexts, may be ill-suited to the particular challenges of evaluating eHealth programs, especially in politicised situations where goals and success criteria are contested.</p>
</list-item><list-item>
<p>We offer an alternative set of guiding principles for eHealth evaluation based on traditions that view evaluation as social practice rather than as scientific testing, and illustrate these with the example of England's controversial Summary Care Record program.</p>
</list-item><list-item>
<p>We invite <italic>PLoS Medicine</italic> readers to join a debate on the relative merits of “scientific” and “social practice” approaches to evaluation and consider the extent to which eHealth evaluation is in need of a paradigm shift.</p>
</list-item></list>
</sec></boxed-text><sec id="s2">
<title>Introduction</title>
<p>Much has been written about why electronic health (eHealth) initiatives fail <xref ref-type="bibr" rid="pmed.1000360-Brown1">[1]</xref>–<xref ref-type="bibr" rid="pmed.1000360-Kreps1">[4]</xref>. Less attention has been paid to why <italic>evaluations</italic> of such initiatives fail to deliver the insights expected of them. <italic>PLoS Medicine</italic> has published three papers offering a “robust” and “scientific” approach to eHealth evaluation <xref ref-type="bibr" rid="pmed.1000360-Lilford1">[5]</xref>–<xref ref-type="bibr" rid="pmed.1000360-Bates1">[7]</xref>. One recommended systematically addressing each part of a “chain of reasoning”, at the centre of which was the program's goals <xref ref-type="bibr" rid="pmed.1000360-Catwell1">[6]</xref>. Another proposed a quasi-experimental step-wedge design, in which late adopters of eHealth innovations serve as controls for early adopters <xref ref-type="bibr" rid="pmed.1000360-Lilford1">[5]</xref>. Interestingly, the authors of the empirical study flagged by these authors as an exemplary illustration of the step-wedge design subsequently abandoned it in favour of a largely qualitative case study because they found it impossible to establish anything approaching a controlled experiment in the study's complex, dynamic, and heavily politicised context <xref ref-type="bibr" rid="pmed.1000360-Robertson1">[8]</xref>.</p>
<p>The approach to evaluation presented in the previous <italic>PLoS Medicine</italic> series rests on a set of assumptions that philosophers of science call “positivist” <xref ref-type="bibr" rid="pmed.1000360-Orlikowski1">[9]</xref>: that there is an external reality that can be objectively measured; that phenomena such as “project goals”, “outcomes”, and “formative feedback” can be precisely and unambiguously defined; that facts and values are clearly distinguishable; and that generalisable statements about the relationship between input and output variables are possible.</p>
<p>Alternative approaches to eHealth evaluation are based on very different philosophical assumptions <xref ref-type="bibr" rid="pmed.1000360-Orlikowski1">[9]</xref>. For example,</p>
<list list-type="bullet"><list-item>
<p>“interpretivist” approaches assume a socially constructed reality (i.e., people perceive issues in different ways and assign different values and significance to facts)—hence, reality is never objectively or unproblematically knowable—and that the identity and values of the researcher are inevitably implicated in the research process <xref ref-type="bibr" rid="pmed.1000360-Klein1">[10]</xref>.</p>
</list-item><list-item>
<p>“critical” approaches assume that critical questioning can generate insights about power relationships and interests and that one purpose of evaluation is to ask such questions on behalf of less powerful and potentially vulnerable groups (such as patients) <xref ref-type="bibr" rid="pmed.1000360-Klecun1">[11]</xref>.</p>
</list-item></list>
</sec><sec id="s3">
<title>Beyond Questions of Science</title>
<p>Catwell and Sheikh argue that “health information systems should be evaluated with the same rigor as a new drug or treatment program, otherwise decisions about future deployments of ICT in the health sector may be determined by social, economic, and/or political circumstances, rather than by robust scientific evidence” (<xref ref-type="bibr" rid="pmed.1000360-Catwell1">[6]</xref>, page 1).</p>
<p>In contrast to this view of evaluation as scientific testing, scholars in critical-interpretivist traditions view evaluation as <italic>social practice</italic>—that is, as actively engaging with a social situation and considering how that situation is framed and enacted by participants <xref ref-type="bibr" rid="pmed.1000360-Wagenaar1">[12]</xref>–<xref ref-type="bibr" rid="pmed.1000360-Ramiller1">[20]</xref>. A key quality criterion in such studies is <italic>reflexivity</italic>—consciously thinking about issues such as values, perspectives, relationships, and trust. These traditions reject the assumption that a rigorous evaluation can be exclusively scientific. Rather, they hold that as well as the scientific agenda of factors, variables, and causal relationships, the evaluation must also embrace the emotions, values, and conflicts associated with a program <xref ref-type="bibr" rid="pmed.1000360-Schwandt1">[19]</xref>. eHealth “interventions” may lie in the technical and scientific world, but eHealth dreams, visions, policies, and programs have personal, social, political, and ideological components, and therefore typically prove fuzzy, slippery, and unstable when we seek to define and control them <xref ref-type="bibr" rid="pmed.1000360-Greenhalgh1">[21]</xref>.</p>
<p>Kushner observes that “The [positivist evaluation] model is elegant in its simplicity, appealing for its rationality, reasonable in asking little more than that people do what they say they will do, and efficient in its economical definition of what data count” (<xref ref-type="bibr" rid="pmed.1000360-Kushner1">[18]</xref>, page 16). But he goes on to list various shortcomings (summarised below), which were illustrated in our evaluation of a nationally stored electronic Summary Care Record (SCR) in England <xref ref-type="bibr" rid="pmed.1000360-Greenhalgh1">[21]</xref>,<xref ref-type="bibr" rid="pmed.1000360-Greenhalgh2">[22]</xref>. The SCR was part of a larger National Programme for IT in the National Health Service <xref ref-type="bibr" rid="pmed.1000360-Brennan1">[23]</xref>, viewed by many stakeholders as monolithic, politically driven, and inflexible <xref ref-type="bibr" rid="pmed.1000360-Kreps1">[4]</xref>,<xref ref-type="bibr" rid="pmed.1000360-Robertson1">[8]</xref>.</p>
<p>The first problem with scientific evaluation, suggests Kushner, is that programs typically have multiple and contested goals; hence, no single set of goals can serve as a fixed referent for comparison. An early finding of our evaluation was that the SCR program had numerous goals (e.g., politicians were oriented to performance and efficiency targets, doctors saw the main goal as improving clinical quality in out-of-hours care, and civil liberties lobbyists perceived the program an attempt by the state to encroach on individual privacy) <xref ref-type="bibr" rid="pmed.1000360-Greenhalgh1">[21]</xref>.</p>
<p>Second, outcomes are not stable; they erode and change over time and across contexts. In the SCR program, it was originally planned that patients would access their electronic record from home via linked software called HealthSpace, thereby becoming “empowered”. But HealthSpace was subsequently uncoupled from the SCR program because it was deemed “high risk” by civil servants <xref ref-type="bibr" rid="pmed.1000360-Greenhalgh3">[24]</xref>.</p>
<p>Third, Kushner suggests, the causal link between process and outcome is typically interrupted by so many intervening variables as to make it unreliable. In the SCR evaluation, we documented 56 such variables—including training, permissions, physical space, technical interoperability, local policies and protocols, professional sanction, and point-of-care consent <xref ref-type="bibr" rid="pmed.1000360-Greenhalgh1">[21]</xref>.</p>
<p>Fourth, key characteristics of program success may not be articulated in the vocabulary of outcomes and may not yield to measurement. One such dimension of the SCR program was the variable culture of e-governance across different organisations (e.g., the extent to which it was acceptable for staff to forget their passwords or leave machines “logged on” when going to lunch).</p>
<p>Finally, program learning that leads away from initial objectives threatens failure against outcome criteria. In the SCR program, an early finding was that predefined milestones (e.g., number of records created by a target date) were sometimes counterproductive since implementation teams were required to push forward in the absence of full clinical and patient engagement, which sometimes led to strong local resistance. We recommended that these milestones be made locally negotiable. But because critics of the program interpreted missed milestones as evidence of “failure”, policymakers took little heed of this advice.</p>
</sec><sec id="s4">
<title>Beyond Variables</title>
<p>“Scientific” evaluation aims to produce statistical statements about the relationship between abstracted variables such as “IT response times”, “resource use”, and “morbidity/mortality” <xref ref-type="bibr" rid="pmed.1000360-Lilford1">[5]</xref>. But the process of producing such variables may remove essential contextual features that are key to <italic>explaining</italic> the phenomenon under study. Controlled, feature-at-a-time comparisons are vulnerable to repeated decomposition: there are features within features, contingencies within contingencies, and tasks within tasks <xref ref-type="bibr" rid="pmed.1000360-DeSanctis1">[25]</xref>.</p>
<p>Expressing findings as statistical relationships between variables may draw attention away from people taking action <xref ref-type="bibr" rid="pmed.1000360-Ramiller1">[20]</xref>. In the real world of eHealth implementation, designers design, managers manage, trainers train, clinicians deliver care, and auditors monitor performance; people exhibit particular personality traits, express emotions, enact power relationships, and generate and deal with conflict. Technologies also “act” in their own non-human way: for example, they boot up, crash, transmit, compute, aggregate, and permit or deny access. A statistical approach may produce more or less valid and more or less reliable estimates of effect size (and hence a “robust” evaluation), but “When we enter the world of variables, we leave behind the ingredients that are needed to produce a story with the kind of substance and verisimilitude that can give a convincing basis for practical action” (<xref ref-type="bibr" rid="pmed.1000360-Ramiller1">[20]</xref>, page 124).</p>
<p>“Substance” (conveying something that feels real) and “verisimilitude” (something that rings true) are linked to the narrative process, which Karl Weick called “sensemaking” <xref ref-type="bibr" rid="pmed.1000360-Weick1">[26]</xref>, which is essential in a multifaceted program whose goals are contested and whose baseline is continually shifting. Collection and analysis of qualitative and quantitative data help illuminate these complexities rather than produce a single “truth”. The narrative form preferred by social scientists for reporting complex case studies allows tensions and ambiguities to be included as key findings, which may be preferable to expressing the “main” findings as statistical relationships between variables and mentioning inconsistencies as a footnote or not at all. Our final SCR report was written as an extended narrative to capture the multiple conflicting framings and inherent tensions that neither we nor the program's architects could resolve <xref ref-type="bibr" rid="pmed.1000360-Greenhalgh1">[21]</xref>.</p>
</sec><sec id="s5">
<title>Beyond “Independence” and “Objectivity”</title>
<p>MacDonald and Kushner identify three forms of evaluation of government-sponsored programs: bureaucratic, autocratic, and democratic, which represent different levels of independence from the state <xref ref-type="bibr" rid="pmed.1000360-MacDonald1">[27]</xref>. Using this taxonomy, the approach endorsed by the previous <italic>PLoS Medicine</italic> series <xref ref-type="bibr" rid="pmed.1000360-Lilford1">[5]</xref>–<xref ref-type="bibr" rid="pmed.1000360-Bates1">[7]</xref> represents a welcome shift from a bureaucratic model (in which management consultants were commissioned to produce evaluations that directly served political ends) to an autocratic model (in which academic experts use systematic methods to produce objective reports that are published independently). But it falls short of the democratic model—in which evaluators engage, explicitly and reflexively, with the arguments exchanged by different stakeholders about ideas, values, and priorities—to which our own team aspired. “Independence” as defined by the terms of autocratic evaluation (effectively, lack of censorship by the state and peer review by other academics who place politics out of scope) pushes evaluators to resist the very engagement with the issues that policy-relevant insights require.</p>
<p>In sum, critical-interpretivist approaches to evaluation have different quality criteria and generate different kinds of knowledge than “scientific” (quasi-experimental) approaches. These differences are summarised in <xref ref-type="table" rid="pmed-1000360-t001">Tables 1</xref> and <xref ref-type="table" rid="pmed-1000360-t002">2</xref>.</p>
<table-wrap id="pmed-1000360-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pmed.1000360.t001</object-id><label>Table 1</label><caption>
<title>Comparison of Key Quality Principles in Positivist versus Critical-Interpretivist Studies.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pmed-1000360-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pmed.1000360.t001" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="2" rowspan="1">Positivist Studies</td>
<td align="left" colspan="2" rowspan="1">Critical-Interpretive Studies</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Principle</td>
<td align="left" colspan="1" rowspan="1">Explanation</td>
<td align="left" colspan="1" rowspan="1">Principle</td>
<td align="left" colspan="1" rowspan="1">Explanation</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">1. Over-arching principle of statistical inference (relating the sample to the population)</td>
<td align="left" colspan="1" rowspan="1">Research is undertaken on a sample that should be adequately powered and statistically representative of the population from which it is drawn</td>
<td align="left" colspan="1" rowspan="1">1. Over-arching principle of the hermeneutic circle (relating the parts to the whole)</td>
<td align="left" colspan="1" rowspan="1">Human understanding is achieved by iterating between the different parts of a phenomenon and the whole that they form</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">2. Principle of multiple interacting variables</td>
<td align="left" colspan="1" rowspan="1">The relationship between input and output variables is affected by numerous mediating and moderating variables, the complete and accurate measurement of which will capture “context”</td>
<td align="left" colspan="1" rowspan="1">2. Principle of contextualisation</td>
<td align="left" colspan="1" rowspan="1">Observations are context-bound and only make sense when placed in an interpretive narrative that shows how they emerged from a particular social and historical background</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">3. Principle of distance</td>
<td align="left" colspan="1" rowspan="1">Good research involves a clear separation between researcher and the people and organisations on which research is undertaken</td>
<td align="left" colspan="1" rowspan="1">3. Principle of interaction and immersion</td>
<td align="left" colspan="1" rowspan="1">Good research involves engagement and dialogue between researcher and research participants, and immersion in the organisational and social context of the study</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">4. Principle of statistical abstraction and generalisation</td>
<td align="left" colspan="1" rowspan="1">Generalisablity is achieved by demonstrating precision, accuracy and reproducibility of relationships between variables</td>
<td align="left" colspan="1" rowspan="1">4. Principle of theoretical abstraction and generalisation</td>
<td align="left" colspan="1" rowspan="1">Generalisability is achieved by relating particular observations and interpretations to a coherent and plausible theoretical model</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">5. Principle of elimination of bias</td>
<td align="left" colspan="1" rowspan="1">Good research eliminates bias through robust methodological designs (e.g., randomisation, stratification)</td>
<td align="left" colspan="1" rowspan="1">5. Principle of researcher reflexivity</td>
<td align="left" colspan="1" rowspan="1">All research is perspectival. Good research exhibits ongoing reflexivity about how the researchers' own backgrounds, interests, and preconceptions affect the questions posed, data gathered, and interpretations offered</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">6. Principle of a single reality amenable to scientific measurement</td>
<td align="left" colspan="1" rowspan="1">There is one reality which scientists may access, provided they use the right study designs, methods, and instruments</td>
<td align="left" colspan="1" rowspan="1">6. Principle of multiple interpretations</td>
<td align="left" colspan="1" rowspan="1">All complex social phenomena are open to multiple interpretations. “Success criteria” and “findings” will be contested. Good research identifies and explores these multiple “truths”.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">7. Principle of empiricism</td>
<td align="left" colspan="1" rowspan="1">There is a direct relationship between what is measured and underlying reality, subject to the robustness of the methods and the precision and accuracy of the instruments</td>
<td align="left" colspan="1" rowspan="1">7. Principle of critical questioning</td>
<td align="left" colspan="1" rowspan="1">The “truth” is not what it appears to be. Critical questioning may generate insights about hidden political influences and domination. Ethical research includes a duty to ask such questions on behalf of vulnerable or less powerful groups.</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt101"><label/><p>Adapted from <xref ref-type="bibr" rid="pmed.1000360-Klein1">[10]</xref>.</p></fn></table-wrap-foot></table-wrap><table-wrap id="pmed-1000360-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pmed.1000360.t002</object-id><label>Table 2</label><caption>
<title>Different Kinds of Knowledge Generated by Different Kinds of Evaluation.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pmed-1000360-t002-2" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pmed.1000360.t002" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">Positivist Evaluations</td>
<td align="left" colspan="1" rowspan="1">Critical-Interpretive Evaluations</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Focuses on objective methods oriented to the collection of “formal knowledge” as data, thereby producing:</td>
<td align="left" colspan="1" rowspan="1">Focuses on naturalistic methods that may capture both formal and informal (tacit, embodied, practical) knowledge, and also co-create learning through dialogue between stakeholders, thereby producing:</td>
</tr>
<tr>
<td colspan="1" rowspan="1">• Quantitative estimates of the relationship between predefined input and output variables, and confidence intervals around these</td>
<td align="left" colspan="1" rowspan="1">• Map of the different stakeholders and insights into their expectations, values, and framings of the program; illumination of who is accountable to whom</td>
</tr>
<tr>
<td colspan="1" rowspan="1">• Deconstruction of “context” to produce quantitative estimates and/or qualitative explanations of the effect of mediating and moderating variables on the relationship between input and output variables</td>
<td align="left" colspan="1" rowspan="1">• Problematisation of “success”; insights into the struggle between stakeholder groups to define and judge success and whose voices are dominant in this struggle</td>
</tr>
<tr>
<td colspan="1" rowspan="1">• Judgement of the extent to which a program has achieved its original goals and the contribution of different elements in the original chain of reasoning to this</td>
<td align="left" colspan="1" rowspan="1">• Illumination of how the eHealth technology exacerbates (or, perhaps, helps overcome) power differentials between different groups (e.g., through differential exposure to surveillance or access to data)</td>
</tr>
<tr>
<td colspan="1" rowspan="1">• Statistical generalisation, allowing prediction of how well a particular eHealth technology is likely to work in other contexts and settings</td>
<td align="left" colspan="1" rowspan="1">• A rich, contextualised narrative that conveys the multiple perspectives on the program and its complex interdependencies and ambiguities•</td>
</tr>
<tr>
<td colspan="1" rowspan="1">• Quantification of how evaluators' formative feedback has influenced outcome</td>
<td align="left" colspan="1" rowspan="1"> Theoretical generalisation, allowing potentially transferable explanations of the dynamic and reciprocal relationship between macro-, meso-, and micro-level influences</td>
</tr>
<tr>
<td colspan="1" rowspan="1">• “Endpoint” knowledge with evaluation methods providing the means to the “end” of producing judgements in a final evaluation report</td>
<td align="left" colspan="1" rowspan="1">• Reflections on how formative feedback and the relationship between evaluators and evaluands may have influenced the program, hence advice to future evaluators on how to manage these relationships</td>
</tr>
<tr>
<td colspan="1" rowspan="1">• Explanatory and predictive knowledge</td>
<td align="left" colspan="1" rowspan="1">• Understanding and illumination</td>
</tr>
</tbody>
</table></alternatives></table-wrap></sec><sec id="s6">
<title>An Alternative Set of Guiding Principles for eHealth Evaluation</title>
<p>Lilford et al. identify four “tricky questions” in eHealth evaluation (qualitative or quantitative?; patient or system?; formative or summative?; internal or external?) and resolve these by recommending mixed-method, patient-and-system studies in which internal evaluations (undertaken by practitioners and policymakers) are formative and external ones (undertaken by “impartial” researchers) are summative <xref ref-type="bibr" rid="pmed.1000360-Lilford1">[5]</xref>. In our view, the tricky questions are more philosophical and political than methodological and procedural.</p>
<p>We offer below an alternative (and at this stage, provisional) set of principles, initially developed to guide our evaluation of the SCR program <xref ref-type="bibr" rid="pmed.1000360-Greenhalgh2">[22]</xref>,<xref ref-type="bibr" rid="pmed.1000360-Greenhalgh4">[28]</xref>, which we invite others to critique, test, and refine. These principles are deliberately presented in a somewhat abstracted and generalised way, since they will need to be applied flexibly with attention to the particularities and contingencies of different contexts and settings. Each principle will be more or less relevant to a particular project, and their relative importance will differ in different evaluations.</p>
<p>First, think about your own role in the evaluation. Try to strike a balance between critical distance on the one hand and immersion and engagement on the other. Ask questions such as What am I investigating—and on whose behalf? How do I balance my obligations to the various institutions and individuals involved? Who owns the data I collect? <xref ref-type="bibr" rid="pmed.1000360-Kushner2">[29]</xref>.</p>
<p>Second, put in place a governance process (including a broad-based advisory group with an independent chair) that formally recognises that there are multiple stakeholders and that power is unevenly distributed between them. Map out everyone's expectations of the program and the evaluation. Be clear that simply because a sponsor pays for an evaluation it does not have special claim on its services or exemption from its focus <xref ref-type="bibr" rid="pmed.1000360-Simons1">[30]</xref>.</p>
<p>Third, provide the interpersonal and analytic space for effective dialogue (e.g., by offering to feed back anonymised data from one group of stakeholders to another). Conversation and debate is not simply a means to an end, it can be an end in itself. Learning happens more through the processes of evaluation than from the final product of an evaluation report <xref ref-type="bibr" rid="pmed.1000360-Widdershoven1">[31]</xref>.</p>
<p>Fourth, take an emergent approach. An evaluation cannot be designed at the outset and pursued relentlessly to its conclusions; it must grow and adapt in response to findings and practical issues which arise in fieldwork. Build theory from emerging data, not the other way round (for example, instead of seeking to test a predefined “causal chain of reasoning”, explore such links by observing social practices).</p>
<p>Fifth, consider the dynamic macro-level context (economic, political, demographic, technological) in which the eHealth innovation is being introduced <xref ref-type="bibr" rid="pmed.1000360-Greenhalgh4">[28]</xref>. Your stakeholder map and challenges of putting together your advisory group should form part of this dataset.</p>
<p>Sixth, consider the different meso-level contexts (e.g., organisations, professional groups, networks), how action plays out in these settings (e.g., in terms of culture, strategic decisions, expectations of staff, incentives, rewards) and how this changes over time. Include reflections on the research process (e.g., gaining access) in this dataset.</p>
<p>Seventh, consider the individuals (e.g., clinicians, managers, service users) through whom the eHealth innovation(s) will be adopted, deployed, and used. Explore their backgrounds, identities and capabilities; what the technology means to them and what they think will happen if and when they use it.</p>
<p>Eighth, consider the eHealth technologies, the expectations and constraints inscribed in them (e.g., access controls, decision models) and how they “work” or not in particular conditions of use. Expose conflicts and ambiguities (e.g., between professional codes of practice and the behaviours expected by technologies).</p>
<p>Ninth, use narrative as an analytic tool and to synthesise findings. Analyse a sample of small-scale incidents in detail to unpack the complex ways in which macro- and meso-level influences impact on technology use at the front line. When writing up the case study, the story form will allow you to engage with the messiness and unpredictability of the program; make sense of complex interlocking events; treat conflicting findings (e.g., between the accounts of top management and staff) as higher-order data; and open up space for further interpretation and deliberation.</p>
<p>Finally, consider critical events in relation to the evaluation itself. Document systematically stakeholders' efforts to re-draw the boundaries of the evaluation, influence the methods, contest the findings, amend the language, modify the conclusions, and delay or suppress publication.</p>
</sec><sec id="s7">
<title>Conclusion</title>
<p>eHealth initiatives often occur in a complex and fast-moving socio-political arena. The tasks of generating, authorising, and disseminating evidence on the success of these initiatives do not occur in a separate asocial and apolitical bubble. They are often produced by, and in turn feed back into, the political process of deciding priorities and allocating resources to pursue them <xref ref-type="bibr" rid="pmed.1000360-House1">[17]</xref>,<xref ref-type="bibr" rid="pmed.1000360-Schwandt1">[19]</xref>. The dispassionate scientist pursuing universal truths may add less value to such a situation than the engaged scholar interpreting practice in context <xref ref-type="bibr" rid="pmed.1000360-Schwandt1">[19]</xref>,<xref ref-type="bibr" rid="pmed.1000360-VandeVen1">[32]</xref>.</p>
<p>Differences in underlying philosophical position may lead to opposing quality criteria for “robust” evaluations. Some eHealth initiatives will lend themselves to scientific evaluation based mainly or even entirely on positivist assumptions, but others, particularly those that are large-scale, complex, politically driven, and differently framed by different stakeholders, may require evaluators to reject these assumptions and apply alternative criteria for rigour <xref ref-type="bibr" rid="pmed.1000360-Patton2">[33]</xref>,<xref ref-type="bibr" rid="pmed.1000360-Contandriopoulos1">[34]</xref>. The precise balance between “scientific” and “alternative” approaches will depend on the nature and context of the program and probably cannot be stipulated in advance. An informed debate on ways of knowing in eHealth evaluation is urgently needed. We offer this paper to open it.</p>
</sec></body>
<back><ref-list>
<title>References</title>
<ref id="pmed.1000360-Brown1"><label>1</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Brown</surname><given-names>AD</given-names></name>
<name name-style="western"><surname>Jones</surname><given-names>MR</given-names></name>
</person-group>             <year>1998</year>             <article-title>Doomed to failure: narratives of inevitability and conspiracy in a failed IS project.</article-title>             <source>Organization Studies</source>             <volume>19</volume>             <fpage>73</fpage>             <lpage>88</lpage>          </element-citation></ref>
<ref id="pmed.1000360-Heeks1"><label>2</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Heeks</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Mundy</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Salazar</surname><given-names>A</given-names></name>
</person-group>             <year>1999</year>             <article-title>Why health care information systems succeed or fail.</article-title>             <comment>Information Systems for Public Sector Management Working Paper Series. Institute for Development Policy and Management: University of Manchester Available: <ext-link ext-link-type="uri" xlink:href="http://www.sed.manchester.ac.uk/idpm/research/publications/wp/igovernment/igov_wp09.htm" xlink:type="simple">http://www.sed.manchester.ac.uk/idpm/research/publications/wp/igovernment/igov_wp09.htm</ext-link>. Accessed 27 September 2010</comment>          </element-citation></ref>
<ref id="pmed.1000360-Scott1"><label>3</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Scott</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Rundall</surname><given-names>TG</given-names></name>
<name name-style="western"><surname>Vogt</surname><given-names>TM</given-names></name>
<name name-style="western"><surname>Hsu</surname><given-names>J</given-names></name>
</person-group>             <year>2007</year>             <source>Implementing an electronic medical record system: successes, failures, lessons</source>             <publisher-loc>Oxford</publisher-loc>             <publisher-name>Radcliffe</publisher-name>          </element-citation></ref>
<ref id="pmed.1000360-Kreps1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kreps</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Richardson</surname><given-names>H</given-names></name>
</person-group>             <year>2007</year>             <article-title>IS success and failure - the problem of scale.</article-title>             <source>The Political Quarterly</source>             <volume>78</volume>             <fpage>439</fpage>             <lpage>446</lpage>          </element-citation></ref>
<ref id="pmed.1000360-Lilford1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lilford</surname><given-names>RJ</given-names></name>
<name name-style="western"><surname>Foster</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Pringle</surname><given-names>M</given-names></name>
</person-group>             <year>2009</year>             <article-title>Evaluating eHealth: how to make evaluation more methodologically robust.</article-title>             <source>PLoS Med</source>             <volume>6</volume>             <fpage>e1000186</fpage>             <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pmed.1000186" xlink:type="simple">10.1371/journal.pmed.1000186</ext-link></comment>          </element-citation></ref>
<ref id="pmed.1000360-Catwell1"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Catwell</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Sheikh</surname><given-names>A</given-names></name>
</person-group>             <year>2009</year>             <article-title>Evaluating eHealth interventions: the need for continuous systemic evaluation.</article-title>             <source>PLoS Med</source>             <volume>6</volume>             <fpage>e1000126</fpage>             <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pmed.1000126" xlink:type="simple">10.1371/journal.pmed.1000126</ext-link></comment>          </element-citation></ref>
<ref id="pmed.1000360-Bates1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bates</surname><given-names>DW</given-names></name>
<name name-style="western"><surname>Wright</surname><given-names>A</given-names></name>
</person-group>             <year>2009</year>             <article-title>Evaluating eHealth: undertaking robust international cross-cultural eHealth research.</article-title>             <source>PLoS Med</source>             <volume>6</volume>             <fpage>e1000105</fpage>             <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pmed.1000105" xlink:type="simple">10.1371/journal.pmed.1000105</ext-link></comment>          </element-citation></ref>
<ref id="pmed.1000360-Robertson1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Robertson</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Cresswell</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Takian</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Petrakaki</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Crowe</surname><given-names>S</given-names></name>
<etal/></person-group>             <year>2010</year>             <article-title>Implementation and adoption of nationwide electronic health records in secondary care in England: qualitative analysis of interim results from a prospective national evaluation.</article-title>             <source>BMJ</source>             <volume>341</volume>             <fpage>c4564</fpage>          </element-citation></ref>
<ref id="pmed.1000360-Orlikowski1"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Orlikowski</surname><given-names>WJ</given-names></name>
<name name-style="western"><surname>Baroudi</surname><given-names>JJ</given-names></name>
</person-group>             <year>1991</year>             <article-title>Studying information technology in organizations: research approaches and assumptions.</article-title>             <source>Information Systems Research</source>             <volume>2</volume>             <fpage>1</fpage>             <lpage>28</lpage>          </element-citation></ref>
<ref id="pmed.1000360-Klein1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Klein</surname><given-names>HK</given-names></name>
<name name-style="western"><surname>Myers</surname><given-names>MD</given-names></name>
</person-group>             <year>1999</year>             <article-title>A set of principles for conducting and evaluating interpretive field studies in information systems.</article-title>             <source>Mis Quarterly</source>             <volume>23</volume>             <fpage>67</fpage>             <lpage>93</lpage>          </element-citation></ref>
<ref id="pmed.1000360-Klecun1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Klecun</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Cornford</surname><given-names>T</given-names></name>
</person-group>             <year>2005</year>             <article-title>A critical approach to evaluation.</article-title>             <source>European Journal of Information Systems</source>             <volume>14</volume>             <fpage>229</fpage>             <lpage>243</lpage>          </element-citation></ref>
<ref id="pmed.1000360-Wagenaar1"><label>12</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wagenaar</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Cook</surname><given-names>SDN</given-names></name>
</person-group>             <year>2003</year>             <article-title>Understanding policy practices: Action, dialectic and deliberation in policy analysis.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Hajer</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Wagenaar</surname><given-names>H</given-names></name>
</person-group>             <source>Deliberative policy analysis. Understanding governance in the network society</source>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>Cambridge University Press</publisher-name>             <fpage>139</fpage>             <lpage>171</lpage>          </element-citation></ref>
<ref id="pmed.1000360-Cicourel1"><label>13</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Cicourel</surname><given-names>A</given-names></name>
</person-group>             <year>1964</year>             <source>Method and measurement in sociology</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Free Press</publisher-name>          </element-citation></ref>
<ref id="pmed.1000360-Patton1"><label>14</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Patton</surname><given-names>MQ</given-names></name>
</person-group>             <year>1997</year>             <source>Utilization-focused evaluation: The new century. Third edition</source>             <publisher-loc>London</publisher-loc>             <publisher-name>Sage</publisher-name>          </element-citation></ref>
<ref id="pmed.1000360-Pawson1"><label>15</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pawson</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Tilley</surname><given-names>N</given-names></name>
</person-group>             <year>1997</year>             <source>Realistic evaluation</source>             <publisher-loc>London</publisher-loc>             <publisher-name>Sage</publisher-name>          </element-citation></ref>
<ref id="pmed.1000360-Flyvbjerg1"><label>16</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Flyvbjerg</surname><given-names>B</given-names></name>
</person-group>             <year>2001</year>             <source>Making social science matter: why social inquiry fails and how it can succeed again</source>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>Cambridge University Press</publisher-name>          </element-citation></ref>
<ref id="pmed.1000360-House1"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>House</surname><given-names>ER</given-names></name>
</person-group>             <year>2006</year>             <article-title>Democracy and evaluation.</article-title>             <source>Evaluation</source>             <volume>12</volume>             <fpage>119</fpage>          </element-citation></ref>
<ref id="pmed.1000360-Kushner1"><label>18</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kushner</surname><given-names>SI</given-names></name>
</person-group>             <year>2002</year>             <article-title>The object of one's passion: engagement and community in democratic evaluation.</article-title>             <source>Evaluation Journal of Australasia</source>             <volume>2</volume>             <fpage>16</fpage>             <lpage>22</lpage>          </element-citation></ref>
<ref id="pmed.1000360-Schwandt1"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schwandt</surname><given-names>TA</given-names></name>
</person-group>             <year>2003</year>             <article-title>Back to the rough ground! Beyond theory to practice in evaluation.</article-title>             <source>Evaluation</source>             <volume>9</volume>             <fpage>353</fpage>          </element-citation></ref>
<ref id="pmed.1000360-Ramiller1"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ramiller</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Pentland</surname><given-names>B</given-names></name>
</person-group>             <year>2009</year>             <article-title>Management implications in information systems research: the untold story.</article-title>             <source>Journal of the Association for Information Systems</source>             <volume>10</volume>             <fpage>474</fpage>             <lpage>494</lpage>          </element-citation></ref>
<ref id="pmed.1000360-Greenhalgh1"><label>21</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Greenhalgh</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Stramer</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Bratan</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Byrne</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Russell</surname><given-names>J</given-names></name>
<etal/></person-group>             <year>2010</year>             <source>The devil's in the detail: final report of the independent evaluation of the Summary Care Record and HealthSpace programmes</source>             <publisher-loc>London</publisher-loc>             <publisher-name>University College London</publisher-name>          </element-citation></ref>
<ref id="pmed.1000360-Greenhalgh2"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Greenhalgh</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Stramer</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Bratan</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Byrne</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Russell</surname><given-names>J</given-names></name>
<etal/></person-group>             <year>2010</year>             <article-title>Adoption and non-adoption of a shared electronic summary record in England.</article-title>             <source>BMJ</source>             <volume>340</volume>             <fpage>c311</fpage>          </element-citation></ref>
<ref id="pmed.1000360-Brennan1"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Brennan</surname><given-names>S</given-names></name>
</person-group>             <year>2007</year>             <article-title>The biggest computer programme in the world ever! How's it going?</article-title>             <source>Journal of Information Technology</source>             <volume>22</volume>             <fpage>202</fpage>             <lpage>211</lpage>          </element-citation></ref>
<ref id="pmed.1000360-Greenhalgh3"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Greenhalgh</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Hinder</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Stramer</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Bratan</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Russell</surname><given-names>J</given-names></name>
</person-group>             <year>2010</year>             <article-title>HealthSpace: case study of the adoption, non-adoption and abandonment of an Internet-accessible personal health organiser.</article-title>             <source>BMJ</source>             <volume>341</volume>             <comment>In press</comment>          </element-citation></ref>
<ref id="pmed.1000360-DeSanctis1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>DeSanctis</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Poole</surname><given-names>MS</given-names></name>
</person-group>             <year>1994</year>             <article-title>Capturing the complexity in advanced technology use: adaptive structuration theory.</article-title>             <source>Organization Science</source>             <volume>5</volume>             <fpage>121</fpage>             <lpage>147</lpage>          </element-citation></ref>
<ref id="pmed.1000360-Weick1"><label>26</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Weick</surname><given-names>KE</given-names></name>
</person-group>             <year>1990</year>             <article-title>Technology as equivoque: sensemaking in new technologies.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Goodman</surname><given-names>PS</given-names></name>
<name name-style="western"><surname>Sproull</surname><given-names>LS</given-names></name>
</person-group>             <source>Technology and organizations</source>             <publisher-loc>San Francisco</publisher-loc>             <publisher-name>Jossey-Bass</publisher-name>             <fpage>1</fpage>             <lpage>44</lpage>          </element-citation></ref>
<ref id="pmed.1000360-MacDonald1"><label>27</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>MacDonald</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Kushner</surname><given-names>S</given-names></name>
</person-group>             <year>2004</year>             <article-title>Democratic evaluation.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Mathison</surname><given-names>S</given-names></name>
</person-group>             <source>Encyclopedia of evaluation</source>             <publisher-loc>Thousand Oaks</publisher-loc>             <publisher-name>Sage</publisher-name>          </element-citation></ref>
<ref id="pmed.1000360-Greenhalgh4"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Greenhalgh</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Stones</surname><given-names>R</given-names></name>
</person-group>             <year>2010</year>             <article-title>Theorising big IT programmes in healthcare: strong structuration theory meets actor-network theory.</article-title>             <source>Social Science &amp; Medicine</source>             <volume>70</volume>             <fpage>1285</fpage>             <lpage>1294</lpage>          </element-citation></ref>
<ref id="pmed.1000360-Kushner2"><label>29</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kushner</surname><given-names>S</given-names></name>
</person-group>             <year>2000</year>             <source>Personalizing evaluation</source>             <publisher-loc>London</publisher-loc>             <publisher-name>Sage</publisher-name>          </element-citation></ref>
<ref id="pmed.1000360-Simons1"><label>30</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Simons</surname><given-names>H</given-names></name>
</person-group>             <year>1987</year>             <source>Getting to know schools in a democracy</source>             <publisher-loc>Lewes</publisher-loc>             <publisher-name>The Falmer Press</publisher-name>          </element-citation></ref>
<ref id="pmed.1000360-Widdershoven1"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Widdershoven</surname><given-names>G</given-names></name>
</person-group>             <year>2001</year>             <article-title>Dialogue in evaluation: a hermeneutic perspective.</article-title>             <source>Evaluation</source>             <volume>7</volume>             <fpage>253</fpage>             <lpage>63</lpage>          </element-citation></ref>
<ref id="pmed.1000360-VandeVen1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Van de Ven</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Johnson</surname><given-names>PE</given-names></name>
</person-group>             <year>2006</year>             <article-title>Knowledge for theory and practice.</article-title>             <source>Academy of Management Review</source>             <volume>31</volume>             <fpage>802</fpage>             <lpage>821</lpage>          </element-citation></ref>
<ref id="pmed.1000360-Patton2"><label>33</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Patton</surname><given-names>MQ</given-names></name>
</person-group>             <year>2010</year>             <source>Developmental evaluation: applying complexity concepts to enhance innovation and use</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Guilford Press</publisher-name>          </element-citation></ref>
<ref id="pmed.1000360-Contandriopoulos1"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Contandriopoulos</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Lemire</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Denis</surname><given-names>J-L</given-names></name>
<name name-style="western"><surname>Tremblay</surname><given-names>É</given-names></name>
</person-group>             <year>2010</year>             <article-title>Knowledge exchange processes in organizations and policy arenas: a narrative systematic review of the literature.</article-title>             <source>Milbank Q</source>             <volume>88</volume>             <comment>In press</comment>          </element-citation></ref>
</ref-list><glossary><title>Abbreviations</title><def-list><def-item>
<term>eHealth</term>
<def>
<p>electronic health</p>
</def>
</def-item><def-item>
<term>SCR</term>
<def>
<p>Summary Care Record</p>
</def>
</def-item></def-list></glossary>
<fn-group><fn fn-type="other"><p><bold>Provenance:</bold> Not commissioned; externally peer-reviewed.</p></fn></fn-group>
</back>
</article>