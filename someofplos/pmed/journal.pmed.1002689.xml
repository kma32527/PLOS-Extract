<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="other" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Med</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosmed</journal-id>
<journal-title-group>
<journal-title>PLOS Medicine</journal-title>
</journal-title-group>
<issn pub-type="ppub">1549-1277</issn>
<issn pub-type="epub">1549-1676</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pmed.1002689</article-id>
<article-id pub-id-type="publisher-id">PMEDICINE-D-18-03354</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Perspective</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Information technology</subject><subj-group><subject>Data processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Machine learning algorithms</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Machine learning algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject><subj-group><subject>Machine learning algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Health care</subject><subj-group><subject>Health care providers</subject><subj-group><subject>Allied health care professionals</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Medical ethics</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and places</subject><subj-group><subject>Population groupings</subject><subj-group><subject>Professions</subject><subj-group><subject>Medical personnel</subject><subj-group><subject>Medical doctors</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Health care</subject><subj-group><subject>Health care providers</subject><subj-group><subject>Medical doctors</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Software engineering</subject><subj-group><subject>Software development</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Software engineering</subject><subj-group><subject>Software development</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Bioengineering</subject><subj-group><subject>Biotechnology</subject><subj-group><subject>Medical devices and equipment</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Bioengineering</subject><subj-group><subject>Biotechnology</subject><subj-group><subject>Medical devices and equipment</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Medical devices and equipment</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>Machine learning in medicine: Addressing ethical challenges</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1303-5467</contrib-id>
<name name-style="western">
<surname>Vayena</surname>
<given-names>Effy</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5908-2002</contrib-id>
<name name-style="western">
<surname>Blasimme</surname>
<given-names>Alessandro</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Cohen</surname>
<given-names>I. Glenn</given-names>
</name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Health Ethics and Policy Lab, Department of Health Sciences and Technology, ETH Zurich, Zurich, Switzerland</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Harvard Law School, Cambridge, Massachusetts, United States of America</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>I have read the journal’s policy and the authors of this manuscript have the following competing interests: EV has received speaking fees from SwissRe, Novartis R&amp;D Academy, and Google Netherlands. IGC served as a consultant for Otsuka Pharmaceuticals advising on the use of digital medicine for its Abilify MyCite product. IGC is supported by the Collaborative Research Program for Biomedical Innovation Law, which is a scientifically independent collaborative research program supported by Novo Nordisk Foundation. AB served as a consultant for Celgene Corporation for the preparation of a workshop on pharmaceutical innovation and received honoraria from SwissRe for participating at an internal event on genome editing.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">effy.vayena@hest.ethz.ch</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>6</day>
<month>11</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<month>11</month>
<year>2018</year>
</pub-date>
<volume>15</volume>
<issue>11</issue>
<elocation-id>e1002689</elocation-id>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Vayena et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pmed.1002689"/>
<abstract abstract-type="toc">
<p>Effy Vayena and colleagues argue that machine learning in medicine must offer data protection, algorithmic transparency, and accountability to earn the trust of patients and clinicians.</p>
</abstract>
<funding-group>
<funding-statement>The authors received no specific funding for this work.</funding-statement>
</funding-group>
<counts>
<fig-count count="1"/>
<table-count count="0"/>
<page-count count="4"/>
</counts>
</article-meta>
</front>
<body>
<p>A recent United Kingdom survey reports that 63% of the adult population is uncomfortable with allowing personal data to be used to improve healthcare and is unfavorable to artificial intelligence (AI) systems replacing doctors and nurses in tasks they usually perform [<xref ref-type="bibr" rid="pmed.1002689.ref001">1</xref>]. Another study, conducted in Germany, found that medical students—the doctors of tomorrow—overwhelmingly buy into the promise of AI to improve medicine (83%) but are more skeptical that it will establish conclusive diagnoses in, for instance, imaging exams (56% disagree) [<xref ref-type="bibr" rid="pmed.1002689.ref002">2</xref>]. When asked about the prospects of AI, United States decision-makers at healthcare organizations are confident that it will improve medicine, but roughly half of them think it will produce fatal errors, will not work properly, and will not meet currently hyped expectations [<xref ref-type="bibr" rid="pmed.1002689.ref003">3</xref>]. These survey data resonate to the ethical and regulatory challenges that surround AI in healthcare, particularly privacy, data fairness, accountability, transparency, and liability. Successfully addressing these will foster the future of machine learning in medicine (MLm) and its positive impact on healthcare. Ethical and regulatory concerns about MLm can be grouped into three broad categories according to how and where they emerge, namely, around the sources of data needed for MLm and the approaches used in the development and deployment of MLm in clinical practice (for a hypothetical case study, see <xref ref-type="fig" rid="pmed.1002689.g001">Fig 1</xref>).</p>
<fig id="pmed.1002689.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pmed.1002689.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Imagine a medical software company developing a machine learning–based device.</title>
<p>The device performs fully automated analysis of histopathology slides from cancer patients and predicts genetic mutations in tumors solely based on these images. This inferred genetic information can be used either for prognostic purposes or to detect an indication for a targeted therapy. Users will not know which features of the images the algorithm associates with mutated genes or the biological explanation for these associations. The selling propositions of the device are that it can infer valuable genetic information early in the diagnostic process and be used in contexts in which genetic testing is not available by analyzing images shared by pathologists on a cloud-based platform. MLm, machine learning in medicine.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pmed.1002689.g001" xlink:type="simple"/>
</fig>
<sec id="sec001">
<title>Data sourcing for MLm must adhere to data protection and privacy requirements</title>
<p>MLm algorithms use data that are subject to privacy protections, requiring that developers pay close attention to ethical and regulatory restrictions at each stage of data processing. Data provenance and consent for use and reuse are of particular importance [<xref ref-type="bibr" rid="pmed.1002689.ref004">4</xref>,<xref ref-type="bibr" rid="pmed.1002689.ref005">5</xref>], especially for MLm that requires considerable amounts and large varieties of data. It is very likely that such disparate data will have different conditions of use and/or be bound by different legal protections. A prominent example is the newly enacted European General Data Protection Regulation (GDPR), which sets out specific informed consent requirements for data uses and grants data subjects several rights that must be respected by those processing their data [<xref ref-type="bibr" rid="pmed.1002689.ref006">6</xref>]. Moreover, this law applies to data from residents of the European Union (EU) irrespective of where the data are processed. Data that are used to train algorithms must have the necessary use authorizations, but determining which data uses are permitted for a given purpose is not an easy feat. This will also depend on data type, jurisdiction, purpose of use, and oversight models.</p>
<p>Ethical and regulatory concerns around health data have been debated for some time, and regulation such as GDPR is emblematic of legislative motivation to address novel digital challenges. However, different jurisdictions have reached and can be expected to reach different conclusions. Compare, for example, the GDPR with the Health Insurance Portability and Accountability Act [HIPAA] in the US, which focuses on healthcare data from patient records but does not cover other sources of health data such as health data generated outside of covered entities and business associates, e.g., data generated by life insurance companies or by a blood glucose–monitoring smartphone app [<xref ref-type="bibr" rid="pmed.1002689.ref007">7</xref>].</p>
</sec>
<sec id="sec002">
<title>MLm development should be committed to fairness</title>
<p>The computer science adage goes, “garbage in, garbage out.” This is especially true for MLm, since the data sets on which MLm models are trained and validated are essential in ensuring the ethical use of predictive algorithms. Poorly representative training data sets can introduce biases into MLm-trained algorithms. “Bias” is a fraught term, with at least two archetypes common in medical data. First are cases in which the data sources themselves do not reflect true epidemiology within a given demographic, as for instance in population data biased by the entrenched overdiagnosis of schizophrenia in African Americans [<xref ref-type="bibr" rid="pmed.1002689.ref008">8</xref>]. Second are cases in which an algorithm is trained on a data set that does not contain enough members of a given demographic—for instance, an algorithm trained mostly on data from older white men. Such an algorithm would make poor predictions, for example, among younger black women. If algorithms trained on data sets with these characteristics are adopted in healthcare, they have the potential to exacerbate health disparities [<xref ref-type="bibr" rid="pmed.1002689.ref009">9</xref>].</p>
<p>To avoid this pitfall, scientific societies and regulatory agencies must develop best practices for recognizing and minimizing the downstream effects of biased training data sets, while bodies such as institutional review boards, ethics review committees, and health technology assessment organizations should check for compliance with such standards. As an example of one opportunity to reduce the risks of biased algorithms, the US Food and Drug Administration (FDA), in the context of its Digital Health Innovation Action Plan, has started a precertification pilot program to assess medical software under development based on five excellence criteria, including quality [<xref ref-type="bibr" rid="pmed.1002689.ref010">10</xref>]. To avert the harms of biased training sets, the FDA quality criterion—and other regulatory criteria like it—could be expanded to cover the risk of bias in training data sets. This need is being addressed in some circles; for instance, the recent American Medical Association (AMA) policy recommendations on AI (in this case, “augmented intelligence”) promote the identification and minimization of biases in data [<xref ref-type="bibr" rid="pmed.1002689.ref011">11</xref>]. A different approach, exemplified by the All of Us precision medicine research cohort in the US, is to fund the development of more representative data sets that can be used for training and validation [<xref ref-type="bibr" rid="pmed.1002689.ref012">12</xref>].</p>
<p>In addition to regulatory protections against algorithmic bias, modernized regulatory approval—in the form of ad hoc guidance—is needed to maintain the safety and efficacy of MLm-based algorithms. These algorithms can in principle improve continuously with each use, resulting in real-time “updates” that cannot possibly be tested individually in clinical trials or assessed according to the typical timetable of a healthcare regulator. Regulators must develop standard procedures including effective postmarket monitoring mechanisms by which developers can transparently document the evolution of their MLm software.</p>
</sec>
<sec id="sec003">
<title>The deployment of MLm should satisfy transparency standards</title>
<p>Perhaps the MLm raising the most difficult ethical and legal questions—and the greatest challenge to current modes of regulation—is represented by noninterpretable, so-called black-box algorithms, the inner logic of which remains hidden even to their developers. This lack of transparency can preclude the mechanistic interpretation of MLm-based assessments and, in turn, reduce their trustworthiness. Moreover, the disclosure of basic yet meaningful details about medical treatment to patients—a fundamental tenet of medical ethics—requires that the doctors themselves grasp at least the fundamental inner workings of the devices they use. Therefore, for MLm to be ethical, developers must communicate to their end users—doctors—the general logic behind MLm-based decisions. Some degree of explainability may also be required to justify the clinical validation of MLm in prospective studies and randomized clinical trials. In the case of fully automated medical decisions, the level of risk associated with the procedure may determine whether and how information should be provided to patients about the presence of MLm-based technologies employed to guide their care. Communicating with patients about the use of MLm technologies may increase their trust and acceptance, which the survey data discussed above suggest is an ongoing concern.</p>
<p>As more diagnostic and therapeutic interventions become based on MLm, the autonomy of patients in decisional processes about their health and the possibility of shared decision-making may be undermined. This would happen, for instance, if reliance on automated decision-making tools reduces the opportunity of meaningful dialogue between healthcare providers and patients or if payers consider MLm recommendations as a precondition for reimbursement and refuse to cover treatments when the MLm recommends against them. Given the importance of personal contact and interaction between patients, healthcare professionals, and caregivers, it is key for healthcare providers to embed MLm in supportive and empowering delivery systems. Third-party auditing may also prove necessary, and such ethical integration may become a precondition for accreditation.</p>
<p>Special attention should be paid to the AI-enabled consumer informatics such as smartphone apps and wearable devices. Rapid progress in machine learning could result in a proliferation of health-related consumer products for which quality standards and accreditation systems should be developed.</p>
<p>Finally, the allocation and grounds for liability for adverse events related to the use of MLm will need to be clarified. Without a thorough ability to predict how various kinds of MLm will expose hospitals, physicians, and developers to liability, it will be hard to achieve widespread adoption [<xref ref-type="bibr" rid="pmed.1002689.ref013">13</xref>,<xref ref-type="bibr" rid="pmed.1002689.ref014">14</xref>].</p>
</sec>
<sec id="sec004" sec-type="conclusions">
<title>Conclusions</title>
<p>The clinical use of MLm may transform existing modes of healthcare delivery. MLm will be used in the clinical setting by healthcare professionals, be embedded in smart devices through the internet of things, and be used by patients themselves beyond the clinical setting for disease self-management of chronic conditions. The exponential growth of investment in MLm signals that research is accelerating, and more products may soon be targeting market entry. To merit the trust of patients and adoption by providers, MLm must fully align with data protection requirements, minimize the effects of bias, be effectively regulated, and achieve transparency. Addressing such ethical and regulatory issues as soon as possible is essential for avoiding unnecessary risks and pitfalls that will hinder further progress of MLm.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="other" id="fn001">
<p><bold>Provenance:</bold> Commissioned; not externally peer reviewed.</p>
</fn>
</fn-group>
<glossary>
<title>Abbreviations</title>
<def-list>
<def-item><term>AI</term>
<def><p>artificial intelligence</p></def>
</def-item>
<def-item><term>AMA</term>
<def><p>American Medical Association</p></def>
</def-item>
<def-item><term>EU</term>
<def><p>European Union</p></def>
</def-item>
<def-item><term>FDA</term>
<def><p>Food and Drug Administration</p></def>
</def-item>
<def-item><term>GDPR</term>
<def><p>General Data Protection Regulation</p></def>
</def-item>
<def-item><term>HIPAA</term>
<def><p>Health Insurance Portability and Accountability Act</p></def>
</def-item>
<def-item><term>MLm</term>
<def><p>machine learning in medicine</p></def>
</def-item>
</def-list>
</glossary>
<ref-list>
<title>References</title>
<ref id="pmed.1002689.ref001"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Fenech M, Strukelj N, Buston O. Ethical, social and polictical challenges of artificial intelligence in health. 2018 April [Cited 19 Sept 2018]. <ext-link ext-link-type="uri" xlink:href="http://futureadvocacy.com/wp-content/uploads/2018/04/1804_26_FA_ETHICS_08-DIGITAL.pdf" xlink:type="simple">http://futureadvocacy.com/wp-content/uploads/2018/04/1804_26_FA_ETHICS_08-DIGITAL.pdf</ext-link>.</mixed-citation></ref>
<ref id="pmed.1002689.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pinto dos Santos</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Giese</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Brodehl</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Chon</surname> <given-names>SH</given-names></name>, <name name-style="western"><surname>Staab</surname> <given-names>W</given-names></name> <etal>et al</etal>. <article-title>Medical students’ attitude towards artificial intelligence: a multicentre survey</article-title>. <source>Eur Radiol</source> <year>2018</year> <month>Jul</month> <day>6</day>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00330-018-5601-1" xlink:type="simple">10.1007/s00330-018-5601-1</ext-link></comment> <object-id pub-id-type="pmid">29980928</object-id></mixed-citation></ref>
<ref id="pmed.1002689.ref003"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Intel Corporation. Overcoming barriers in AI adoption in healthcare. 2018 April [Cited Sept 19, 2018]. <ext-link ext-link-type="uri" xlink:href="https://newsroom.intel.com/wp-content/uploads/sites/11/2018/07/healthcare-iot-infographic.pdf" xlink:type="simple">https://newsroom.intel.com/wp-content/uploads/sites/11/2018/07/healthcare-iot-infographic.pdf</ext-link>.</mixed-citation></ref>
<ref id="pmed.1002689.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vayena</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Blasimme</surname> <given-names>A</given-names></name>. <article-title>Biomedical big data: new models of control over access, use and governance</article-title>. <source>J Bioeth Inq</source>. <year>2017</year>;<volume>14</volume>:<fpage>501</fpage>–<lpage>513</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s11673-017-9809-6" xlink:type="simple">10.1007/s11673-017-9809-6</ext-link></comment> <object-id pub-id-type="pmid">28983835</object-id></mixed-citation></ref>
<ref id="pmed.1002689.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vayena</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Blasimme</surname> <given-names>A</given-names></name>. <article-title>Health research with big data: Time for systemic oversight</article-title>. <source>The Journal of Law, Medicine &amp; Ethics</source>. <year>2018</year>; <volume>46</volume>: <fpage>119</fpage>–<lpage>129</lpage>.</mixed-citation></ref>
<ref id="pmed.1002689.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McCall</surname> <given-names>B</given-names></name>. <article-title>What does the GDPR means for the medical community?</article-title> <source>The Lancet</source>. <year>2018</year>; <volume>391</volume>: <fpage>1249</fpage>–<lpage>50</lpage>.</mixed-citation></ref>
<ref id="pmed.1002689.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cohen</surname> <given-names>IG</given-names></name>, <name name-style="western"><surname>Mello</surname> <given-names>MM</given-names></name>. <article-title>HIPAA and protecting health information in the 21st Century</article-title>. <source>JAMA</source>. <year>2018</year>;<volume>320</volume>(<issue>3</issue>):<fpage>231</fpage>–<lpage>232</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1001/jama.2018.5630" xlink:type="simple">10.1001/jama.2018.5630</ext-link></comment> <object-id pub-id-type="pmid">29800120</object-id></mixed-citation></ref>
<ref id="pmed.1002689.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Neighbors</surname> <given-names>HW</given-names></name>, <name name-style="western"><surname>Jackson</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Campbell</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>D</given-names></name>. <article-title>The influence of racial factors on psychiatric diagnosis: a review and suggestions for research</article-title>. <source>Community Ment Health J</source>. <year>1989</year>;<volume>25</volume>: <fpage>301</fpage>–<lpage>11</lpage>. <object-id pub-id-type="pmid">2697490</object-id></mixed-citation></ref>
<ref id="pmed.1002689.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Braveman</surname> <given-names>P</given-names></name>. <article-title>Health disparities and health equity: concepts and measurement</article-title>. <source>Annu Rev Public Health</source>. <year>2006</year>;<volume>27</volume>:<fpage>167</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev.publhealth.27.021405.102103" xlink:type="simple">10.1146/annurev.publhealth.27.021405.102103</ext-link></comment> <object-id pub-id-type="pmid">16533114</object-id></mixed-citation></ref>
<ref id="pmed.1002689.ref010"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">U.S. Food and Drug Administration. Developing Software Precertification Program: A Working Model. 2018 June [Cited 19 Sept 2018]. <ext-link ext-link-type="uri" xlink:href="https://www.fda.gov/downloads/MedicalDevices/DigitalHealth/DigitalHealthPreCertProgram/UCM611103.pdf" xlink:type="simple">https://www.fda.gov/downloads/MedicalDevices/DigitalHealth/DigitalHealthPreCertProgram/UCM611103.pdf</ext-link>.</mixed-citation></ref>
<ref id="pmed.1002689.ref011"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">The American Medical Association. AMA passes first policy recommendations on augmented intelligence. 2018 Jun 14 [Cited 19 Sept 2018]. <ext-link ext-link-type="uri" xlink:href="https://www.ama-assn.org/ama-passes-first-policy-recommendations-augmented-intelligence" xlink:type="simple">https://www.ama-assn.org/ama-passes-first-policy-recommendations-augmented-intelligence</ext-link>.</mixed-citation></ref>
<ref id="pmed.1002689.ref012"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">National Institutes of Health. All of Us research program. [Cited 28 Sept 2018]. <ext-link ext-link-type="uri" xlink:href="https://allofus.nih.gov/" xlink:type="simple">https://allofus.nih.gov/</ext-link></mixed-citation></ref>
<ref id="pmed.1002689.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Price</surname> <given-names>WN</given-names></name>. <article-title>Regulating black-box medicine</article-title>. <source>Mich L Rev</source>. <year>2017</year>; <volume>116</volume>(<issue>1</issue>): <fpage>421</fpage>–<lpage>74</lpage>.</mixed-citation></ref>
<ref id="pmed.1002689.ref014"><label>14</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Price</surname> <given-names>WN</given-names></name>. <chapter-title>Medical Malpractice and Black-Box Medicine</chapter-title>. In <name name-style="western"><surname>Cohen</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Lynch</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Vayena</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Gasser</surname> <given-names>U</given-names></name>. (Eds.). (<year>2018</year>). <source>Big Data, Health Law, and Bioethics</source>. <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation></ref>
</ref-list>
</back>
</article>